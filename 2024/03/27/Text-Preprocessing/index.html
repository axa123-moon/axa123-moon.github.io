<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"axa123-moon.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Text Preprocessing Tokenization  1.1. 단어 토큰화  1.2. 문장 토큰화 Cleaning Normalization  3.1. Stemming  3.2. Lemmatization Edit Distance  1. Tokenization 주어진 데이터를 토큰(Token)이라 불리는 단위로 나누는 작업  1.1  단어 토큰화12345">
<meta property="og:type" content="article">
<meta property="og:title" content="Text Preprocessing">
<meta property="og:url" content="https://axa123-moon.github.io/2024/03/27/Text-Preprocessing/index.html">
<meta property="og:site_name" content="AI Beginner&#39;s Blog">
<meta property="og:description" content="Text Preprocessing Tokenization  1.1. 단어 토큰화  1.2. 문장 토큰화 Cleaning Normalization  3.1. Stemming  3.2. Lemmatization Edit Distance  1. Tokenization 주어진 데이터를 토큰(Token)이라 불리는 단위로 나누는 작업  1.1  단어 토큰화12345">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-03-27T08:00:31.000Z">
<meta property="article:modified_time" content="2024-03-27T08:53:31.615Z">
<meta property="article:author" content="axa123">
<meta property="article:tag" content="패스트캠퍼스">
<meta property="article:tag" content="UpstageAILab">
<meta property="article:tag" content="Upstage">
<meta property="article:tag" content="부트캠프">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="데이터사이언스">
<meta property="article:tag" content="무료교육">
<meta property="article:tag" content="국비지원">
<meta property="article:tag" content="Text Preprocessing">
<meta property="article:tag" content="Tokenization">
<meta property="article:tag" content="Cleaning">
<meta property="article:tag" content="Normalization">
<meta property="article:tag" content="Edit Distance">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://axa123-moon.github.io/2024/03/27/Text-Preprocessing/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://axa123-moon.github.io/2024/03/27/Text-Preprocessing/","path":"2024/03/27/Text-Preprocessing/","title":"Text Preprocessing"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Text Preprocessing | AI Beginner's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">AI Beginner's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Text-Preprocessing"><span class="nav-number">1.</span> <span class="nav-text">Text Preprocessing</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Tokenization"><span class="nav-number">1.1.</span> <span class="nav-text">1. Tokenization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%EB%8B%A8%EC%96%B4-%ED%86%A0%ED%81%B0%ED%99%94"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1  단어 토큰화</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%EB%AC%B8%EC%9E%A5-%ED%86%A0%ED%81%B0%ED%99%94"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2  문장 토큰화</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Cleaning"><span class="nav-number">1.2.</span> <span class="nav-text">2. Cleaning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Normalization"><span class="nav-number">1.3.</span> <span class="nav-text">3. Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Stemming-%EC%96%B4%EA%B0%84-%EC%B6%94%EC%B6%9C"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1. Stemming (어간 추출)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-Lemmatization-%EC%9B%90%ED%98%95-%EC%B6%94%EC%B6%9C"><span class="nav-number">1.4.</span> <span class="nav-text">3.2. Lemmatization (원형 추출)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Edit-Distance"><span class="nav-number">1.5.</span> <span class="nav-text">4. Edit Distance</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">axa123</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">25</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">75</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://axa123-moon.github.io/2024/03/27/Text-Preprocessing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="axa123">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AI Beginner's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Text Preprocessing | AI Beginner's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Text Preprocessing
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2024-03-27 17:00:31 / Modified: 17:53:31" itemprop="dateCreated datePublished" datetime="2024-03-27T17:00:31+09:00">2024-03-27</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="Text-Preprocessing"><a href="#Text-Preprocessing" class="headerlink" title="Text Preprocessing"></a>Text Preprocessing</h1><ol>
<li>Tokenization<br>  1.1. 단어 토큰화<br>  1.2. 문장 토큰화</li>
<li>Cleaning</li>
<li>Normalization<br>  3.1. Stemming<br>  3.2. Lemmatization</li>
<li>Edit Distance</li>
</ol>
<h2 id="1-Tokenization"><a href="#1-Tokenization" class="headerlink" title="1. Tokenization"></a>1. Tokenization</h2><ul>
<li>주어진 데이터를 토큰(Token)이라 불리는 단위로 나누는 작업</li>
</ul>
<h3 id="1-1-단어-토큰화"><a href="#1-1-단어-토큰화" class="headerlink" title="1.1  단어 토큰화"></a>1.1  단어 토큰화</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download(<span class="string">&#x27;punkt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> WordPunctTokenizer</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> TreebankWordTokenizer</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">&quot;Although we may not be fully aware of natural language, we need to master several preprocessing techniques to perform natural language processing.&quot;</span></span><br><span class="line"></span><br><span class="line">word_tokenize(text)</span><br></pre></td></tr></table></figure>

<p>[‘Although’,<br> ‘we’,<br> ‘may’,<br> ‘not’,<br> ‘be’,<br> ‘fully’,<br> ‘aware’,<br> ‘of’,<br> ‘natural’,<br> ‘language’,<br> ‘,’,<br> ‘we’,<br> ‘need’,<br> ‘to’,<br> ‘master’,<br> ‘several’,<br> ‘preprocessing’,<br> ‘techniques’,<br> ‘to’,<br> ‘perform’,<br> ‘natural’,<br> ‘language’,<br> ‘processing’,<br> ‘.’]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">punct_tokenizer = WordPunctTokenizer() </span><br><span class="line">punct_tokenizer.tokenize(text)</span><br></pre></td></tr></table></figure>
<p>[‘Although’,<br> ‘we’,<br> ‘may’,<br> ‘not’,<br> ‘be’,<br> ‘fully’,<br> ‘aware’,<br> ‘of’,<br> ‘natural’,<br> ‘language’,<br> ‘,’,<br> ‘we’,<br> ‘need’,<br> ‘to’,<br> ‘master’,<br> ‘several’,<br> ‘preprocessing’,<br> ‘techniques’,<br> ‘to’,<br> ‘perform’,<br> ‘natural’,<br> ‘language’,<br> ‘processing’,<br> ‘.’]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tree_tokenizer = TreebankWordTokenizer()</span><br><span class="line">tree_tokenizer.tokenize(text)</span><br></pre></td></tr></table></figure>

<p>[‘Although’,<br> ‘we’,<br> ‘may’,<br> ‘not’,<br> ‘be’,<br> ‘fully’,<br> ‘aware’,<br> ‘of’,<br> ‘natural’,<br> ‘language’,<br> ‘,’,<br> ‘we’,<br> ‘need’,<br> ‘to’,<br> ‘master’,<br> ‘several’,<br> ‘preprocessing’,<br> ‘techniques’,<br> ‘to’,<br> ‘perform’,<br> ‘natural’,<br> ‘language’,<br> ‘processing’,<br> ‘.’]</p>
<h3 id="1-2-문장-토큰화"><a href="#1-2-문장-토큰화" class="headerlink" title="1.2  문장 토큰화"></a>1.2  문장 토큰화</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentences = <span class="string">&quot;NLP stands for Natural Language Processing. \</span></span><br><span class="line"><span class="string">It is a branch of artificial intelligence that focuses on the interaction between computers and human languages. \</span></span><br><span class="line"><span class="string">NLP combines techniques from linguistics, computer science, and machine learning to enable computers to understand and interpret. \</span></span><br><span class="line"><span class="string">And it creates human language in a meaningful and useful way.&quot;</span></span><br><span class="line"></span><br><span class="line">nltk.sent_tokenize(sentences)</span><br></pre></td></tr></table></figure>

<p>[‘NLP stands for Natural Language Processing.’,<br> ‘It is a branch of artificial intelligence that focuses on the interaction between computers and human languages.’,<br> ‘NLP combines techniques from linguistics, computer science, and machine learning to enable computers to understand and interpret.’,<br> ‘And it creates human language in a meaningful and useful way.’]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sentences = <span class="string">&quot;I am actively looking for Ph.D. students. and you are a Ph.D student.&quot;</span></span><br><span class="line">nltk.sent_tokenize(sentences)</span><br></pre></td></tr></table></figure>

<p>[‘I am actively looking for Ph.D. students.’, ‘and you are a Ph.D student.’]</p>
<h2 id="2-Cleaning"><a href="#2-Cleaning" class="headerlink" title="2. Cleaning"></a>2. Cleaning</h2><ul>
<li>토큰화 작업에 방해가 되거나 의미가 없는 부분의 텍스트, 노이즈를 제거</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line">nltk.download(<span class="string">&#x27;stopwords&#x27;</span>)</span><br><span class="line"></span><br><span class="line">stop=<span class="built_in">set</span>(stopwords.words(<span class="string">&#x27;english&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(stop)</span><br></pre></td></tr></table></figure>

<p>{‘i’, ‘don’, ‘just’, ‘been’, ‘into’, ‘ve’, ‘should’, ‘this’, “shouldn’t”, ‘yours’, ‘but’, “aren’t”, ‘your’, ‘about’, “needn’t”, ‘m’, ‘itself’, ‘other’, ‘wasn’, “wouldn’t”, ‘too’, ‘haven’, “doesn’t”, “mightn’t”, ‘hasn’, ‘didn’, ‘once’, ‘and’, “you’ll”, ‘won’, ‘we’, ‘his’, ‘both’, ‘needn’, “you’re”, ‘do’, ‘how’, ‘me’, ‘further’, ‘ain’, ‘an’, ‘isn’, ‘herself’, “wasn’t”, ‘their’, “you’ve”, ‘yourself’, ‘against’, ‘while’, ‘down’, ‘had’, ‘up’, ‘over’, ‘any’, ‘him’, ‘being’, ‘you’, ‘than’, ‘out’, “hasn’t”, ‘more’, ‘off’, ‘can’, “it’s”, ‘is’, ‘during’, ‘aren’, ‘the’, “haven’t”, ‘then’, ‘these’, ‘them’, ‘be’, ‘shan’, ‘whom’, ‘has’, ‘under’, ‘such’, ‘ll’, ‘no’, ‘all’, ‘they’, ‘through’, ‘am’, “you’d”, “weren’t”, ‘wouldn’, ‘with’, ‘in’, ‘my’, ‘on’, ‘that’, ‘if’, ‘was’, ‘why’, ‘ours’, ‘did’, ‘only’, “she’s”, ‘her’, ‘own’, ‘mightn’, ‘mustn’, ‘y’, ‘s’, ‘not’, “isn’t”, ‘where’, ‘few’, ‘by’, ‘our’, ‘so’, ‘most’, ‘of’, ‘because’, ‘have’, ‘re’, ‘as’, ‘are’, “shan’t”, ‘having’, ‘a’, ‘ourselves’, ‘which’, ‘to’, ‘from’, ‘each’, ‘or’, ‘until’, ‘t’, ‘who’, ‘d’, ‘there’, ‘doesn’, ‘himself’, ‘what’, ‘here’, ‘hers’, ‘at’, “didn’t”, ‘hadn’, ‘were’, ‘below’, ‘very’, ‘ma’, ‘now’, ‘it’, ‘those’, ‘some’, ‘themselves’, “that’ll”, ‘does’, ‘shouldn’, ‘between’, ‘when’, ‘after’, ‘myself’, ‘theirs’, ‘doing’, ‘again’, ‘o’, ‘yourselves’, ‘for’, “won’t”, “hadn’t”, ‘same’, “mustn’t”, ‘she’, ‘nor’, ‘above’, “should’ve”, ‘couldn’, “couldn’t”, ‘he’, ‘will’, “don’t”, ‘weren’, ‘its’, ‘before’}</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sentence=<span class="string">&quot;I want to go shopping and buy snacks&quot;</span></span><br><span class="line">tokens=nltk.word_tokenize(sentence)</span><br><span class="line"></span><br><span class="line">clean_tokens=[]</span><br><span class="line"><span class="keyword">for</span> tok <span class="keyword">in</span> tokens:</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">len</span>(tok.lower())&gt;<span class="number">1</span> <span class="keyword">and</span> (tok.lower() <span class="keyword">not</span> <span class="keyword">in</span> stop): </span><br><span class="line">    clean_tokens.append(tok) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;정상 문장: &quot;</span>,tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;불용어 제거: &quot;</span>,clean_tokens)</span><br></pre></td></tr></table></figure>

<p>정상 문장:  [‘I’, ‘want’, ‘to’, ‘go’, ‘shopping’, ‘and’, ‘buy’, ‘snacks’]<br>불용어 제거:  [‘want’, ‘go’, ‘shopping’, ‘buy’, ‘snacks’]</p>
<h2 id="3-Normalization"><a href="#3-Normalization" class="headerlink" title="3. Normalization"></a>3. Normalization</h2><ul>
<li>의미가 중복되거나 의미론적으로 유사한 단어들을 하나로 통합하거나, 단어의 원형을 찾아 통일해주는 작업</li>
</ul>
<h3 id="3-1-Stemming-어간-추출"><a href="#3-1-Stemming-어간-추출" class="headerlink" title="3.1. Stemming (어간 추출)"></a>3.1. Stemming (어간 추출)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"></span><br><span class="line">stem = PorterStemmer() <span class="comment"># stemmer 선언</span></span><br><span class="line">text = <span class="string">&quot;This was not the map we found on Billy Bonds&#x27; chest, but the exact copy of everything, name and height and sound, except for the red cross and the written note.&quot;</span></span><br><span class="line">words = word_tokenize(text)</span><br><span class="line"><span class="built_in">print</span>(words)</span><br></pre></td></tr></table></figure>

<p>[‘This’, ‘was’, ‘not’, ‘the’, ‘map’, ‘we’, ‘found’, ‘on’, ‘Billy’, ‘Bonds’, “‘“, ‘chest’, ‘,’, ‘but’, ‘the’, ‘exact’, ‘copy’, ‘of’, ‘everything’, ‘,’, ‘name’, ‘and’, ‘height’, ‘and’, ‘sound’, ‘,’, ‘except’, ‘for’, ‘the’, ‘red’, ‘cross’, ‘and’, ‘the’, ‘written’, ‘note’, ‘.’]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> words:</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span> --&gt; <span class="subst">&#123;stem.stem(i)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>This –&gt; thi<br>was –&gt; wa<br>not –&gt; not<br>the –&gt; the<br>map –&gt; map<br>we –&gt; we<br>found –&gt; found<br>on –&gt; on<br>Billy –&gt; billi<br>Bonds –&gt; bond<br>‘ –&gt; ‘<br>chest –&gt; chest<br>, –&gt; ,<br>but –&gt; but<br>the –&gt; the<br>exact –&gt; exact<br>copy –&gt; copi<br>of –&gt; of<br>everything –&gt; everyth<br>, –&gt; ,<br>name –&gt; name<br>and –&gt; and<br>height –&gt; height<br>and –&gt; and<br>sound –&gt; sound<br>, –&gt; ,<br>except –&gt; except<br>for –&gt; for<br>the –&gt; the<br>red –&gt; red<br>cross –&gt; cross<br>and –&gt; and<br>the –&gt; the<br>written –&gt; written<br>note –&gt; note<br>. –&gt; .</p>
<h2 id="3-2-Lemmatization-원형-추출"><a href="#3-2-Lemmatization-원형-추출" class="headerlink" title="3.2. Lemmatization (원형 추출)"></a>3.2. Lemmatization (원형 추출)</h2><ul>
<li>문장 속에서 다양한 형태로 활용된(inflected) 단어의 표제어(lemma)를 찾는 일</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line">nltk.download(<span class="string">&#x27;wordnet&#x27;</span>)</span><br><span class="line"></span><br><span class="line">l = WordNetLemmatizer() </span><br><span class="line">words = [<span class="string">&#x27;doing&#x27;</span>, <span class="string">&#x27;has&#x27;</span>, <span class="string">&#x27;going&#x27;</span>, <span class="string">&#x27;loves&#x27;</span>, <span class="string">&#x27;lives&#x27;</span>, <span class="string">&#x27;flying&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> words:</span><br><span class="line">  lemma = l.lemmatize(i, pos=<span class="string">&#x27;v&#x27;</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span> --&gt; <span class="subst">&#123;lemma&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>doing –&gt; do<br>has –&gt; have<br>going –&gt; go<br>loves –&gt; love<br>lives –&gt; live<br>flying –&gt; fly</p>
<h2 id="4-Edit-Distance"><a href="#4-Edit-Distance" class="headerlink" title="4. Edit Distance"></a>4. Edit Distance</h2><ul>
<li>2개의 문자열이 얼만큼 다른가를 거리개념으로 치환해 숫자로 표현한 것. 삽입, 삭제, 교체 연산이 존재함.</li>
<li>heat 과  hit 두단어의 차이는 2입니다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.metrics <span class="keyword">import</span> edit_distance</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(edit_distance(<span class="string">&quot;heat&quot;</span>,<span class="string">&quot;hit&quot;</span>))</span><br></pre></td></tr></table></figure>
<p>2</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%ED%8C%A8%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%8D%BC%EC%8A%A4/" rel="tag"># 패스트캠퍼스</a>
              <a href="/tags/UpstageAILab/" rel="tag"># UpstageAILab</a>
              <a href="/tags/Upstage/" rel="tag"># Upstage</a>
              <a href="/tags/%EB%B6%80%ED%8A%B8%EC%BA%A0%ED%94%84/" rel="tag"># 부트캠프</a>
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/" rel="tag"># 데이터사이언스</a>
              <a href="/tags/%EB%AC%B4%EB%A3%8C%EA%B5%90%EC%9C%A1/" rel="tag"># 무료교육</a>
              <a href="/tags/%EA%B5%AD%EB%B9%84%EC%A7%80%EC%9B%90/" rel="tag"># 국비지원</a>
              <a href="/tags/Text-Preprocessing/" rel="tag"># Text Preprocessing</a>
              <a href="/tags/Tokenization/" rel="tag"># Tokenization</a>
              <a href="/tags/Cleaning/" rel="tag"># Cleaning</a>
              <a href="/tags/Normalization/" rel="tag"># Normalization</a>
              <a href="/tags/Edit-Distance/" rel="tag"># Edit Distance</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/03/08/ROUGE/" rel="prev" title="ROUGE">
                  <i class="fa fa-angle-left"></i> ROUGE
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/04/01/TF-IDF/" rel="next" title="TF IDF">
                  TF IDF <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">axa123</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
